"""
LLMè°ƒç”¨è®°å½•å™¨ï¼šè®°å½•å’Œæ˜¾ç¤ºæ‰€æœ‰LLM APIè°ƒç”¨
"""
import json
import os
from typing import Dict, List, Optional
from datetime import datetime


class LLMCallLogger:
    """LLMè°ƒç”¨è®°å½•å™¨"""
    
    def __init__(self, log_file: Optional[str] = None):
        self.calls: List[Dict] = []
        self.enabled = True
        self.log_file = log_file or os.path.join(
            os.path.dirname(__file__), 
            "..", 
            "llm_calls.log"
        )
        self.log_file = os.path.abspath(self.log_file)
        self.current_test_module: Optional[str] = None
        self.current_test_name: Optional[str] = None
        self._ensure_log_dir()
    
    def set_test_context(self, test_module: str = None, test_name: str = None):
        """
        è®¾ç½®å½“å‰æµ‹è¯•ä¸Šä¸‹æ–‡
        
        Args:
            test_module: æµ‹è¯•æ¨¡å—åç§°ï¼ˆå¦‚ 'test_agent', 'test_multi_agent_coordinator'ï¼‰
            test_name: æµ‹è¯•æ–¹æ³•åç§°ï¼ˆå¦‚ 'test_agent_with_logging'ï¼‰
        """
        self.current_test_module = test_module
        self.current_test_name = test_name
    
    def log_call(self, platform: str, messages: List[Dict], response: str, 
                 model: str = None, temperature: float = None, usage: Dict = None):
        """
        è®°å½•ä¸€æ¬¡LLMè°ƒç”¨
        
        Args:
            platform: APIå¹³å°ï¼ˆdeepseek/openaiï¼‰
            messages: å‘é€çš„æ¶ˆæ¯åˆ—è¡¨
            response: LLMè¿”å›çš„å“åº”
            model: ä½¿ç”¨çš„æ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°
            usage: APIè¿”å›çš„usageä¿¡æ¯ï¼ˆåŒ…å«çœŸå®tokenæ•°ï¼‰
        """
        if not self.enabled:
            return
        
        # ä¼˜å…ˆä½¿ç”¨APIè¿”å›çš„çœŸå®tokenæ•°ï¼Œå¦åˆ™ä¼°ç®—
        if usage:
            input_tokens = usage.get('prompt_tokens', 0)
            output_tokens = usage.get('completion_tokens', 0)
            total_tokens = usage.get('total_tokens', input_tokens + output_tokens)
        else:
            # è®¡ç®—tokenæ•°ï¼ˆç®€å•ä¼°ç®—ï¼‰
            input_tokens = self._estimate_tokens(messages)
            output_tokens = self._estimate_tokens([{"content": response}])
            total_tokens = input_tokens + output_tokens
        
        call_info = {
            "timestamp": datetime.now().isoformat(),
            "platform": platform,
            "model": model or "unknown",
            "temperature": temperature,
            "test_module": self.current_test_module,
            "test_name": self.current_test_name,
            "input": {
                "messages": messages,
                "token_count": input_tokens,
                "is_estimated": usage is None
            },
            "output": {
                "response": response,
                "token_count": output_tokens,
                "is_estimated": usage is None
            },
            "total_tokens": total_tokens,
            "usage": usage
        }
        
        self.calls.append(call_info)
        self._print_call(call_info)
        self._write_to_file(call_info)
    
    def _ensure_log_dir(self):
        """ç¡®ä¿æ—¥å¿—æ–‡ä»¶ç›®å½•å­˜åœ¨"""
        log_dir = os.path.dirname(self.log_file)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)
    
    def _write_to_file(self, call_info: Dict):
        """å°†è°ƒç”¨ä¿¡æ¯å†™å…¥æ–‡ä»¶"""
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write("\n" + "="*80 + "\n")
                f.write(f"ğŸ“ LLM API è°ƒç”¨ #{len(self.calls)}\n")
                f.write("="*80 + "\n")
                
                # æ·»åŠ æµ‹è¯•ä¿¡æ¯
                if call_info.get('test_module') or call_info.get('test_name'):
                    f.write(f"ğŸ§ª æµ‹è¯•æ¨¡å—: {call_info.get('test_module', 'æœªçŸ¥')}\n")
                    if call_info.get('test_name'):
                        f.write(f"ğŸ§ª æµ‹è¯•æ–¹æ³•: {call_info.get('test_name')}\n")
                    f.write("-"*80 + "\n")
                
                f.write(f"â° æ—¶é—´: {call_info['timestamp']}\n")
                f.write(f"ğŸ”§ å¹³å°: {call_info['platform']}\n")
                f.write(f"ğŸ¤– æ¨¡å‹: {call_info['model']}\n")
                if call_info.get('temperature'):
                    f.write(f"ğŸŒ¡ï¸  æ¸©åº¦: {call_info['temperature']}\n")
                
                token_info = f"{call_info['input']['token_count']} tokens"
                if call_info['input'].get('is_estimated'):
                    token_info += " (ä¼°ç®—)"
                else:
                    token_info += " (APIè¿”å›)"
                f.write(f"\nğŸ“¥ è¾“å…¥ ({token_info}):\n")
                f.write("-"*80 + "\n")
                
                for i, msg in enumerate(call_info['input']['messages'], 1):
                    role = msg.get('role', 'unknown')
                    content = msg.get('content', '')
                    f.write(f"\n[{i}] {role.upper()}:\n")
                    f.write(content + "\n")
                
                token_info = f"{call_info['output']['token_count']} tokens"
                if call_info['output'].get('is_estimated'):
                    token_info += " (ä¼°ç®—)"
                else:
                    token_info += " (APIè¿”å›)"
                f.write(f"\nğŸ“¤ è¾“å‡º ({token_info}):\n")
                f.write("-"*80 + "\n")
                
                output = call_info['output']['response']
                f.write(output + "\n")
                
                f.write(f"\nğŸ“Š æ€»è®¡: {call_info['total_tokens']} tokens\n")
                f.write("="*80 + "\n")
        except Exception as e:
            print(f"âš ï¸  å†™å…¥æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
    
    def _estimate_tokens(self, messages: List[Dict]) -> int:
        """
        ç®€å•ä¼°ç®—tokenæ•°
        ä¸­æ–‡çº¦1.5å­—ç¬¦=1tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦=1token
        """
        total_chars = 0
        for msg in messages:
            content = msg.get("content", "")
            total_chars += len(content)
        
        # æ··åˆä¼°ç®—ï¼šå‡è®¾å¹³å‡2å­—ç¬¦=1token
        return total_chars // 2
    
    def _print_call(self, call_info: Dict):
        """æ‰“å°è°ƒç”¨ä¿¡æ¯"""
        print("\n" + "="*80)
        print(f"ğŸ“ LLM API è°ƒç”¨ #{len(self.calls)}")
        print("="*80)
        
        # æ·»åŠ æµ‹è¯•ä¿¡æ¯
        if call_info.get('test_module') or call_info.get('test_name'):
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å—: {call_info.get('test_module', 'æœªçŸ¥')}")
            if call_info.get('test_name'):
                print(f"ğŸ§ª æµ‹è¯•æ–¹æ³•: {call_info.get('test_name')}")
            print("-"*80)
        
        print(f"â° æ—¶é—´: {call_info['timestamp']}")
        print(f"ğŸ”§ å¹³å°: {call_info['platform']}")
        print(f"ğŸ¤– æ¨¡å‹: {call_info['model']}")
        if call_info.get('temperature'):
            print(f"ğŸŒ¡ï¸  æ¸©åº¦: {call_info['temperature']}")
        token_info = f"{call_info['input']['token_count']} tokens"
        if call_info['input'].get('is_estimated'):
            token_info += " (ä¼°ç®—)"
        else:
            token_info += " (APIè¿”å›)"
        print(f"\nğŸ“¥ è¾“å…¥ ({token_info}):")
        print("-"*80)
        
        for i, msg in enumerate(call_info['input']['messages'], 1):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹ï¼ˆåœ¨åˆé€‚çš„ä½ç½®æˆªæ–­ï¼Œé¿å…æˆªæ–­ä»£ç å—ï¼‰
            if len(content) > 500:
                # å°è¯•åœ¨æ¢è¡Œç¬¦åæˆªæ–­ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç›´æ¥æˆªæ–­
                truncate_pos = 500
                if truncate_pos < len(content):
                    # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„æ¢è¡Œç¬¦
                    for pos in range(truncate_pos, max(0, truncate_pos - 50), -1):
                        if content[pos] == '\n':
                            truncate_pos = pos + 1
                            break
                display_content = content[:truncate_pos] + f"\n\n... [å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå…±{len(content)}å­—ç¬¦]"
            else:
                display_content = content
            print(f"\n[{i}] {role.upper()}:")
            print(display_content)
        
        token_info = f"{call_info['output']['token_count']} tokens"
        if call_info['output'].get('is_estimated'):
            token_info += " (ä¼°ç®—)"
        else:
            token_info += " (APIè¿”å›)"
        print(f"\nğŸ“¤ è¾“å‡º ({token_info}):")
        print("-"*80)
        output = call_info['output']['response']
        if len(output) > 500:
            # å°è¯•åœ¨æ¢è¡Œç¬¦åæˆªæ–­
            truncate_pos = 500
            if truncate_pos < len(output):
                # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„æ¢è¡Œç¬¦
                for pos in range(truncate_pos, max(0, truncate_pos - 50), -1):
                    if output[pos] == '\n':
                        truncate_pos = pos + 1
                        break
            print(output[:truncate_pos] + f"\n\n... [å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå…±{len(output)}å­—ç¬¦]")
        else:
            print(output)
        
        print(f"\nğŸ“Š æ€»è®¡: {call_info['total_tokens']} tokens")
        print("="*80 + "\n")
    
    def get_summary(self) -> Dict:
        """è·å–è°ƒç”¨æ‘˜è¦"""
        if not self.calls:
            return {
                "total_calls": 0,
                "total_tokens": 0,
                "platforms": {}
            }
        
        total_tokens = sum(call['total_tokens'] for call in self.calls)
        platforms = {}
        for call in self.calls:
            platform = call['platform']
            if platform not in platforms:
                platforms[platform] = {"calls": 0, "tokens": 0}
            platforms[platform]["calls"] += 1
            platforms[platform]["tokens"] += call['total_tokens']
        
        return {
            "total_calls": len(self.calls),
            "total_tokens": total_tokens,
            "platforms": platforms
        }
    
    def print_summary(self):
        """æ‰“å°è°ƒç”¨æ‘˜è¦"""
        summary = self.get_summary()
        summary_text = "\n" + "="*80 + "\n"
        summary_text += "ğŸ“Š LLM API è°ƒç”¨æ‘˜è¦\n"
        summary_text += "="*80 + "\n"
        summary_text += f"æ€»è°ƒç”¨æ¬¡æ•°: {summary['total_calls']}\n"
        summary_text += f"æ€»Tokenæ•°: {summary['total_tokens']}\n"
        summary_text += "\næŒ‰å¹³å°ç»Ÿè®¡:\n"
        for platform, stats in summary['platforms'].items():
            summary_text += f"  {platform}: {stats['calls']} æ¬¡è°ƒç”¨, {stats['tokens']} tokens\n"
        summary_text += "="*80 + "\n"
        summary_text += f"ğŸ“„ å®Œæ•´æ—¥å¿—å·²ä¿å­˜åˆ°: {self.log_file}\n"
        summary_text += "="*80 + "\n"
        
        print(summary_text)
        
        # åŒæ—¶å†™å…¥æ–‡ä»¶
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(summary_text)
        except Exception as e:
            print(f"âš ï¸  å†™å…¥æ‘˜è¦å¤±è´¥: {e}")
    
    def clear(self):
        """æ¸…ç©ºè®°å½•"""
        self.calls.clear()
        self.current_test_module = None
        self.current_test_name = None
        # æ¸…ç©ºæ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé»˜è®¤è¿½åŠ æ¨¡å¼ï¼‰
        # å¦‚æœéœ€è¦æ¯æ¬¡æµ‹è¯•éƒ½æ¸…ç©ºæ–‡ä»¶ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
        # try:
        #     if os.path.exists(self.log_file):
        #         os.remove(self.log_file)
        # except Exception:
        #     pass
    
    def disable(self):
        """ç¦ç”¨è®°å½•"""
        self.enabled = False
    
    def enable(self):
        """å¯ç”¨è®°å½•"""
        self.enabled = True


# å…¨å±€è®°å½•å™¨å®ä¾‹
logger = LLMCallLogger()

